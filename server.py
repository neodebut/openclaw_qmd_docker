import os
from mcp.server.fastmcp import FastMCP
from sentence_transformers import SentenceTransformer
import torch

# åˆå§‹åŒ– FastMCP
mcp = FastMCP("BGE-M3-Memory-Server")

# åŠ è¼‰æ¨¡å‹ (å„ªå…ˆæª¢æŸ¥æœ‰ç„¡ GPU)
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Loading BGE-M3 model on {device}...")
model = SentenceTransformer('BAAI/bge-m3', device=device)

@mcp.tool()
async def generate_embedding(text: str) -> list[float]:
    """å°‡æ–‡å­—è½‰æ›ç‚º 1024 ç¶­åº¦çš„ BGE-M3 å‘é‡"""
    embedding = model.encode(text, normalize_embeddings=True)
    return embedding.tolist()

if __name__ == "__main__":
    # å–å¾— Zeabur å¯èƒ½æä¾›çš„ PORT ç’°å¢ƒè®Šæ•¸ï¼Œè‹¥ç„¡å‰‡é è¨­ç‚º 8080
    port = int(os.environ.get("PORT", 8080))
    
    print(f"ğŸš€ Starting MCP Server on port {port}...")
    
    # åŸ·è¡Œä¼ºæœå™¨ï¼Œä¸¦æ˜ç¢ºç¶å®š 0.0.0.0 èˆ‡ 8080
    mcp.run(
        transport="sse",
        host="0.0.0.0", 
        port=port
    )
